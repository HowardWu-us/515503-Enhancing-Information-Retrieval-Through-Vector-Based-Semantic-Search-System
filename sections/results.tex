\section{Results}
We tested the performance of our semantic search engine using 10 real-world queries. The results were evaluated using the MRR metric, which assesses the rank of the correct result across multiple queries. The system performed well, with queries such as “NVIDIA Alternative Service” and “AI Competition” ranking first with a perfect score of 1.00. Other queries showed mixed results, with an average MRR score of 0.70. A detailed description of the testing methodology can be found in the appendix.

We applied our semantic search engine to the Department of Computer Science's website and developed an Chrome extension to address the limitations of traditional keyword-based search. Furthermore, we extended this technology to Retrieval-Augmented Generation (RAG) and created an add-on capable of responding to questions based on data from Google Docs.
